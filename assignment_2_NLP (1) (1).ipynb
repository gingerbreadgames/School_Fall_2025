{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roMkS49YSocN"
      },
      "source": [
        "# Introduction to NLTK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtg3nWoiSocN"
      },
      "source": [
        "## Part 1 - Analyzing Moby Dick"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('udhr')\n",
        "nltk.download('tagsets_json')\n",
        "\n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "hawQfXnLVfPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dd3473-2433-4cc5-e7ee-24b684a58c09"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/udhr.zip.\n",
            "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "T1WKF2IESocN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "moby_raw = ' '.join(text1)\n",
        "moby_tokens = nltk.word_tokenize(moby_raw)\n",
        "text1 = nltk.Text(moby_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfV9FDGXSocO"
      },
      "source": [
        "### Example 1\n",
        "\n",
        "How many tokens (words and punctuation symbols) are in text1?\n",
        "\n",
        "*This function should return an integer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w4yiAUBaSocO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55fbe8e-5cc4-48c7-e4da-33a6429225ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263333"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def example_one():\n",
        "\n",
        "    return len(nltk.word_tokenize(moby_raw))\n",
        "\n",
        "example_one()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKM_9DlVSocP"
      },
      "source": [
        "### Example 2\n",
        "\n",
        "How many unique tokens (unique words and punctuation) does text1 have?\n",
        "\n",
        "*This function should return an integer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q6iGwFlSocP"
      },
      "outputs": [],
      "source": [
        "def example_two():\n",
        "\n",
        "    return len(set(nltk.word_tokenize(moby_raw)))\n",
        "\n",
        "example_two()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQCEdbVcSocQ"
      },
      "source": [
        "### Example 3\n",
        "\n",
        "After lemmatizing the verbs, how many unique tokens does text1 have?\n",
        "\n",
        "*This function should return an integer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KEcBwjjpSocQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc0bfa8-a1f6-491d-d5b3-c7f9a1ba088f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15419"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def example_three():\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
        "\n",
        "    return len(set(lemmatized))\n",
        "\n",
        "example_three()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I0SPidGSocQ"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
        "\n",
        "*This function should return a float.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p4pk0eZSocQ"
      },
      "outputs": [],
      "source": [
        "def answer_one():\n",
        "  # Your code here:\n",
        "    unique_tokens =\n",
        "    total_tokens =\n",
        "    lexical_diversity =\n",
        "\n",
        "    return lexical_diversity\n",
        "\n",
        "answer_one()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC2L5DZ_SocR"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "What percentage of tokens is 'whale'or 'Whale'?\n",
        "\n",
        "*This function should return a float.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OALMhHUrSocR",
        "outputId": "7b971c89-5628-465b-a49c-3a27f0c9da6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4511398115693817"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from nltk import FreqDist\n",
        "def answer_two():\n",
        "  # Your code here:\n",
        "    distribution = FreqDist(text1)\n",
        "    Whale_freq = distribution['Whale']\n",
        "    whale_freq = distribution['whale']\n",
        "    total_tokens = len(text1)\n",
        "    freq = Whale_freq + whale_freq\n",
        "    freq_percentage = (freq / total_tokens) * 100\n",
        "    return freq_percentage\n",
        "\n",
        "answer_two()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apevKxguSocR"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency? (Using FreqDist function)\n",
        "\n",
        "*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ag3NKVekSocR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c98db2d-a0a8-4a4e-b408-1100b5408995"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 19229),\n",
              " ('the', 13721),\n",
              " ('.', 7514),\n",
              " ('of', 6536),\n",
              " ('and', 6024),\n",
              " ('a', 4569),\n",
              " ('to', 4542),\n",
              " (';', 4173),\n",
              " ('in', 3916),\n",
              " ('that', 2982),\n",
              " (\"'\", 2919),\n",
              " ('-', 2555),\n",
              " ('his', 2459),\n",
              " ('it', 2209),\n",
              " ('I', 2124),\n",
              " ('!', 1767),\n",
              " ('s', 1739),\n",
              " ('--', 1713),\n",
              " ('is', 1695),\n",
              " ('he', 1661)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def answer_three():\n",
        "  # Your code here:\n",
        "\n",
        "    distribution = FreqDist(text1)\n",
        "    return distribution.most_common(20)\n",
        "\n",
        "\n",
        "answer_three()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4ZPEYHSocR"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "What tokens have a length of greater than 5 and frequency of more than 150?\n",
        "\n",
        "*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDU9rBaBSocS"
      },
      "outputs": [],
      "source": [
        "def answer_four():\n",
        "\n",
        "# Your code here:\n",
        "    distribution = FreqDist(text1)\n",
        "    freqwords = [w for w in set(text1) if len(w) > 5 and distribution[w] > 150]\n",
        "    freqwords.sort()\n",
        "\n",
        "    return freqwords\n",
        "\n",
        "answer_four()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz12_HCOSocS"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "Find the longest word in text1 and that word's length.\n",
        "\n",
        "*This function should return a tuple `(longest_word, length)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DmM2jAsmSocS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946da216-1632-4571-dca9-f93a53672745"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('uninterpenetratingly', 20)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def answer_five():\n",
        "# Your code here:\n",
        "    text_list = list(text1)\n",
        "    sorted_list = sorted(text_list, key=len)\n",
        "    longest_word = sorted_list[-1]\n",
        "    return (longest_word, len(longest_word))\n",
        "\n",
        "answer_five()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obldVXkgSocS"
      },
      "source": [
        "### Question 6\n",
        "\n",
        "What unique words have a frequency of more than 2000? What is their frequency?\n",
        "\n",
        "\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n",
        "\n",
        "*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GQhbELFfSocS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b864921-98e1-4b42-e162-e6ed2e54817b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(13721, 'the'),\n",
              " (6536, 'of'),\n",
              " (6024, 'and'),\n",
              " (4569, 'a'),\n",
              " (4542, 'to'),\n",
              " (3916, 'in'),\n",
              " (2982, 'that'),\n",
              " (2459, 'his'),\n",
              " (2209, 'it'),\n",
              " (2124, 'I')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def answer_six():\n",
        "# Your code here:\n",
        "    distribution = FreqDist(text1)\n",
        "    words = [w for w in set(text1) if w.isalpha() and distribution[w] > 2000]\n",
        "    freqwords = [(distribution[w], w) for w in words]\n",
        "    sorted_words = sorted(freqwords, reverse=True)\n",
        "\n",
        "    return sorted_words\n",
        "\n",
        "answer_six()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81l7JQglSocS"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "What is the average number of tokens per sentence?\n",
        "\n",
        "*This function should return a float.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aRdEAO0_SocT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a385d60c-3494-4e97-85c9-a8d7628b9bf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.314879584290995"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import nltk\n",
        "def answer_seven():\n",
        "# Your code here:\n",
        "    sentences = nltk.sent_tokenize(moby_raw)\n",
        "    number_of_sentences = len(sentences)\n",
        "    tokens_per_sentence = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "    total_tokens = sum(tokens_per_sentence)\n",
        "    average_tokens_per_sentence = total_tokens / number_of_sentences\n",
        "\n",
        "    return average_tokens_per_sentence\n",
        "\n",
        "answer_seven()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FczoCyiSocT"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
        "\n",
        "*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a-2omCIfSocT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b96de00-f122-4ffd-e50a-676cb6da314f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NN', 35651), ('IN', 28891), ('DT', 25870), (',', 19229), ('JJ', 17962)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from nltk import pos_tag, FreqDist\n",
        "\n",
        "def answer_eight():\n",
        "  # Your code here:\n",
        "    pos_tags = pos_tag(text1)\n",
        "    tag_counts = FreqDist(tag for (word, tag) in pos_tags)\n",
        "    most_common_tags = tag_counts.most_common(5)\n",
        "    return most_common_tags\n",
        "\n",
        "answer_eight()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMelCSDTSocV"
      },
      "source": [
        "## Basic NLP Tasks with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g7VMWg3XSocV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP9_odEVSocY"
      },
      "source": [
        "### Normalization and stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "INaoMh_WSocY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed73e19-5dd4-4a35-d042-b85a13dc5845"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list', 'listed', 'lists', 'listing', 'listings']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "input1 = \"List listed lists listing listings\"\n",
        "words1 = input1.lower().split(' ')\n",
        "words1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zfN2ERx_SocY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000848bd-79e8-4179-fa86-cc83e673bc22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list', 'list', 'list', 'list', 'list']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "[porter.stem(t) for t in words1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGvKsWg9SocY"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OXv6NrzoSocY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54e233b-9b35-4a5d-a8f4-f185a5888170"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Universal',\n",
              " 'Declaration',\n",
              " 'of',\n",
              " 'Human',\n",
              " 'Rights',\n",
              " 'Preamble',\n",
              " 'Whereas',\n",
              " 'recognition',\n",
              " 'of',\n",
              " 'the',\n",
              " 'inherent',\n",
              " 'dignity',\n",
              " 'and',\n",
              " 'of',\n",
              " 'the',\n",
              " 'equal',\n",
              " 'and',\n",
              " 'inalienable',\n",
              " 'rights',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
        "udhr[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "evXTi85zSocY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad724f8-ae5d-455f-c59b-4d18769757fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['univers',\n",
              " 'declar',\n",
              " 'of',\n",
              " 'human',\n",
              " 'right',\n",
              " 'preambl',\n",
              " 'wherea',\n",
              " 'recognit',\n",
              " 'of',\n",
              " 'the',\n",
              " 'inher',\n",
              " 'digniti',\n",
              " 'and',\n",
              " 'of',\n",
              " 'the',\n",
              " 'equal',\n",
              " 'and',\n",
              " 'inalien',\n",
              " 'right',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "[porter.stem(t) for t in udhr[:20]] # Still Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4onqC0SUSocY"
      },
      "outputs": [],
      "source": [
        "WNlemma = nltk.WordNetLemmatizer()\n",
        "[WNlemma.lemmatize(t) for t in udhr[:20]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj5RQZA6SocY"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7KXBjHDSocY"
      },
      "outputs": [],
      "source": [
        "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
        "text11.split(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-Iu4vo2SocZ"
      },
      "outputs": [],
      "source": [
        "nltk.word_tokenize(text11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usqYv3CsSocZ"
      },
      "outputs": [],
      "source": [
        "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
        "sentences = nltk.sent_tokenize(text12)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChZBSUeYSocZ"
      },
      "outputs": [],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-n2IiZqSocZ"
      },
      "source": [
        "## Advanced NLP Tasks with NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg5uF7oWSocZ"
      },
      "source": [
        "### POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g_TqcNvYSocZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f95dd68-d87f-4dc9-cf60-9fb8a924a29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n"
          ]
        }
      ],
      "source": [
        "nltk.help.upenn_tagset('MD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nilN9VFBSocZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3097851-bbec-46ca-9bf3-6fc05fc599d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('first', 'JJ'),\n",
              " ('sentence', 'NN'),\n",
              " ('.', '.'),\n",
              " ('A', 'DT'),\n",
              " ('gallon', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('milk', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('U.S.', 'NNP'),\n",
              " ('costs', 'VBZ'),\n",
              " ('$', '$'),\n",
              " ('2.99', 'CD'),\n",
              " ('.', '.'),\n",
              " ('Is', 'VBZ'),\n",
              " ('this', 'DT'),\n",
              " ('the', 'DT'),\n",
              " ('third', 'JJ'),\n",
              " ('sentence', 'NN'),\n",
              " ('?', '.'),\n",
              " ('Yes', 'UH'),\n",
              " (',', ','),\n",
              " ('it', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Your code here:\n",
        "# tokenize the word in text 12 and print out the pos_tag of it.\n",
        "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
        "text13 = nltk.word_tokenize(text12)\n",
        "nltk.pos_tag(text13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lz8TT7tDSocZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce0413e-34ca-45a3-d9f2-2447c2451939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
          ]
        }
      ],
      "source": [
        "# Parsing sentence structure\n",
        "text15 = nltk.word_tokenize(\"Alice loves Bob\")\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "VP -> V NP\n",
        "NP -> 'Alice' | 'Bob'\n",
        "V -> 'loves'\n",
        "\"\"\")\n",
        "\n",
        "parser = nltk.ChartParser(grammar)\n",
        "trees = parser.parse_all(text15)\n",
        "for tree in trees:\n",
        "    print(tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IDnJhXm8Soca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919feb23-ae8f-4b0d-8aae-b2d68db8fc7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP-SBJ\n",
            "    (NP (NNP Pierre) (NNP Vinken))\n",
            "    (, ,)\n",
            "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
            "    (, ,))\n",
            "  (VP\n",
            "    (MD will)\n",
            "    (VP\n",
            "      (VB join)\n",
            "      (NP (DT the) (NN board))\n",
            "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
            "      (NP-TMP (NNP Nov.) (CD 29))))\n",
            "  (. .))\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import treebank\n",
        "text16 = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
        "print(text16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WMLf9ggSoca"
      },
      "source": [
        "### POS tagging and parsing ambiguity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HL6TUe_GSoca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d0224c-1434-412d-d809-afb40c8774eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "text18 = nltk.word_tokenize(\"The old man the boat\")\n",
        "nltk.pos_tag(text18)"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "python-text-mining",
      "graded_item_id": "r35En",
      "launcher_item_id": "tCVfW",
      "part_id": "NTVgL"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}