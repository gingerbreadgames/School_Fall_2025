{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXYsrCPOH1RoLq5IpflGtm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1D_UNET"],"metadata":{"id":"baJkyrAehDJd"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74fc2903","executionInfo":{"status":"ok","timestamp":1759349477918,"user_tz":300,"elapsed":45224,"user":{"displayName":"Reid Layne","userId":"03666684710468430964"}},"outputId":"312283d5-5f5f-409f-ab20-330f05fcbef9"},"source":["'''\n","Code sources:\n","https://www.youtube.com/watch?v=ZBKpAp_6TGI&t=1841s\n","https://huggingface.co/blog/annotated-diffusion\n","'''\n","import torch\n","from torch import nn, Tensor, einsum\n","from einops.layers.torch import Rearrange\n","from einops import rearrange\n","\n","class DownBlock(nn.Module):\n","    '''\n","    Downsamples the sequence length by a factor of 2.\n","    If the input sequence is (batch_size x channels x length)\n","    the output sequence will be (batch_size x channels x length // 2)\n","    '''\n","    def __init__(self, in_channels: int, out_channels):\n","        super().__init__()\n","\n","        ## MODIFY ##\n","        # self.rearrange = Rearrange('b c (l p1) -> b (c p1) l', p1 = 2)\n","        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2)\n","        ## MODIFY ##\n","        # self.conv = nn.Conv1d(in_channels * 2, out_channels, kernel_size = 3, padding = 1)\n","        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size = 3, padding = 1)\n","\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        return self.conv(self.rearrange(x))\n","\n","class UpBlock(nn.Module):\n","    '''\n","    Upsamples the sequence length by a factor of 2.\n","    If the input sequence is (batch_size x channels x length)\n","    the output sequence will be (batch_size x channels x length * 2)\n","    '''\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","\n","        ## This can stay the same\n","        self.upsample = nn.Upsample(scale_factor = 2)\n","\n","        ## MODIFY ##\n","        # self.conv = nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 1)\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        return self.conv(self.upsample(x))\n","\n","class Block(nn.Module):\n","    '''\n","    Basic building block for our neural network.\n","    Comprised of a linear layer followed by normalization and activation.\n","    '''\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","\n","        ## MODIFY ##\n","        # self.conv = nn.Conv1d(in_channels, out_channels, kernel_size = 3, padding = 1)\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n","        self.norm = nn.GroupNorm(8, out_channels)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        x = self.norm(self.conv(x))\n","        return self.act(x)\n","\n","class ResBlock(nn.Module):\n","    '''\n","    An implementation of a skip connection.\n","    Otherwise referred to as a Residual Block.\n","    '''\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","\n","        self.block1 = Block(in_channels, out_channels)\n","        self.block2 = Block(out_channels, out_channels)\n","\n","        ## MODIFY ##\n","        # self.skip = nn.Conv1d(in_channels, out_channels, kernel_size = 1)\n","        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n","\n","    def forward(self, x: Tensor):\n","        h = self.block1(x)\n","        return self.block2(h) + self.skip(x)\n","\n","class RMSNorm(nn.Module):\n","    '''\n","    Normalizes the input data by dividing by the root mean square of the data along the channel dimention.\n","    Also includes a learnable scaling parameter.\n","    '''\n","    def __init__(self, in_channels: int):\n","        super().__init__()\n","\n","        ## MODIFY ##\n","        # self.g = nn.Parameter(torch.ones(1, in_channels, 1))\n","        self.g = nn.Parameter(torch.ones(1, in_channels, 1, 1))\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        ## pytorch normalize takes the signal and computes x / sum(x ** 2, dim = 1).sqrt()\n","        ## we want x / (sum(x ** 2, dim = 1) / sqrt(channels))\n","        return nn.functional.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n","\n","## https://huggingface.co/blog/annotated-diffusion\n","class LinearAttention(nn.Module):\n","    '''\n","    Computes attention that scales linearly in memory O(n) rather than polynomial O(n^2).\n","\n","    https://arxiv.org/pdf/2006.16236\n","    '''\n","    def __init__(self, in_channels: int, heads: int, dim_head: int):\n","        super().__init__()\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        hidden_dim = heads * dim_head\n","\n","        self.norm = RMSNorm(in_channels)\n","\n","        ## MODIFY ##\n","        # self.qkv = nn.Conv1d(in_channels, 3*hidden_dim, kernel_size = 1, bias = False)\n","        self.qkv = nn.Conv2d(in_channels, 3*hidden_dim, kernel_size = 1, bias = False)\n","        self.output = nn.Sequential(\n","            ## MODIFY ##\n","            # nn.Conv1d(hidden_dim, in_channels, kernel_size = 1),\n","            nn.Conv2d(hidden_dim, in_channels, kernel_size = 1),\n","            RMSNorm(in_channels)\n","        )\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","\n","        ## MODIFY ##\n","        # b, c, l = x.shape\n","        b, c, height, width = x.shape\n","\n","\n","        x = self.norm(x)\n","        qkv = self.qkv(x)\n","        q, k, v = qkv.chunk(3, dim = 1)\n","\n","        ## takes the channel dimention and reshapes into heads, channels\n","        ## also flattens the feature maps into vectors\n","        ## the shape is now b h c d where d is dim_head\n","\n","        ## MODIFY ##\n","        # q = rearrange(q, 'b (h c) l -> b h c l', h = self.heads)\n","        q = rearrange(q, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","\n","        ## MODIFY ##\n","        # k = rearrange(k, 'b (h c) l -> b h c l', h = self.heads)\n","        k = rearrange(k, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","\n","        ## MODIFY ##\n","        # v = rearrange(v, 'b (h c) l -> b h c l', h = self.heads)\n","        v = rearrange(v, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","\n","        ## softmax along dim_head dim\n","        q = q.softmax(dim = 2) * self.scale\n","        ## softmax along flattened image dim\n","        k = k.softmax(dim = 3)\n","        ## compute comparison betweeen keys and values to produce context.\n","        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n","        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n","\n","        ## MODIFY ##\n","        ## (hint: store the original image height and width and use rearrange(..., i = height, j = width))\n","        # out = rearrange(out, 'b h c l -> b (h c) l')\n","        out = rearrange(out, 'b h c (i j) -> b (h c) i j', i = height, j = width)\n","        return self.output(out)\n","\n","## https://huggingface.co/blog/annotated-diffusion\n","class Attention(nn.Module):\n","    '''\n","    Computes full pixelwise attention.\n","    Every pixel attends to every other pixel.\n","    This operation is very costly memorywise so it is only used on small feature maps.\n","    '''\n","    def __init__(self, in_channels: int, heads: int, dim_head: int):\n","        super().__init__()\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        hidden_dim = heads * dim_head\n","\n","        self.norm = RMSNorm(in_channels)\n","        ## MODIFY ##\n","        # self.qkv = nn.Conv1d(in_channels, hidden_dim * 3, kernel_size = 1, bias = False)\n","        self.qkv = nn.Conv2d(in_channels, hidden_dim * 3, kernel_size = 1, bias = False)\n","        self.output = nn.Sequential(\n","            ## MODIFY ##\n","            # nn.Conv1d(hidden_dim, in_channels, kernel_size = 1),\n","            nn.Conv2d(hidden_dim, in_channels, kernel_size = 1),\n","            RMSNorm(in_channels)\n","        )\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        ## MODIFY ##\n","        # b, c, l = x.shape\n","        b, c, height, width = x.shape\n","\n","        x = self.norm(x)\n","\n","        ## compute the queries, keys, and values of the incoming feature maps\n","        q, k, v = torch.chunk(self.qkv(x), 3, dim = 1)\n","        ## takes the channel dimention and reshapes into heads, channels\n","\n","        ## MODIFY ##\n","        # q = rearrange(q, 'b (h c) l -> b h c l', h = self.heads)\n","        q = rearrange(q, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","\n","        ## MODIFY ##\n","        # k = rearrange(k, 'b (h c) l -> b h c l', h = self.heads)\n","        k = rearrange(k, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","\n","        ## MODIFY ##\n","        # v = rearrange(v, 'b (h c) l -> b h c l', h = self.heads)\n","        v = rearrange(v, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","        q = q * self.scale\n","        ## multiplication of the query and key matrixes for each head\n","        sim = einsum('b h c i, b h c j -> b h i j', q, k)\n","        ## subtract the maximum value of each row\n","        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n","        ## make each row into a probability distribution\n","        attn = sim.softmax(dim = -1)\n","        ## weight the values according to the rows of the attention matrix\n","        y = einsum('b h i j, b h d j -> b h i d', attn, v)\n","        # reshape the weighted values back into feature maps\n","\n","        ## MODIFY ##\n","        # y = rearrange(y, 'b h l d -> b (h d) l')\n","        y = rearrange(y, 'b h (i j) d -> b (h d) i j', i = height, j = width)\n","        return self.output(y)\n","\n","class UNet(nn.Module):\n","    '''\n","    Full UNet implementation for 1d sequences.\n","    For teaching purposes this network only has two downsampling/upsampling stages.\n","    In practice you can (and should) add more layers.\n","    '''\n","    def __init__(self, in_channels: int = 2, heads: int = 4, dim_head: int = 32):\n","        super().__init__()\n","\n","        ## MODIFY ##\n","        # self.input_layer = nn.Conv1d(in_channels, 32, kernel_size = 1)\n","        self.input_layer = nn.Conv2d(in_channels, 32, kernel_size = 1)\n","\n","        self.downs = nn.ModuleList([\n","\n","            ## first downsampling stage\n","            nn.ModuleList([\n","                ResBlock(32, 32),\n","                ResBlock(32, 32),\n","                LinearAttention(32, heads, dim_head),\n","                DownBlock(32, 32)\n","            ]),\n","\n","            ## second downsampling stage\n","            nn.ModuleList([\n","                ResBlock(32, 32),\n","                ResBlock(32, 32),\n","                LinearAttention(32, heads, dim_head),\n","                DownBlock(32, 64)\n","            ]),\n","\n","            ## does not downsample\n","            nn.ModuleList([\n","                ResBlock(64, 64),\n","                ResBlock(64, 64),\n","                LinearAttention(64, heads, dim_head),\n","                ## MODIFY ##\n","                # nn.Conv1d(64, 128, kernel_size = 3, padding = 1)\n","                nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n","            ])\n","        ])\n","\n","        ## core layers which apply full pixelwise attention to the compressed featuremaps\n","        self.mid_block1 = ResBlock(128, 128)\n","        self.mid_attention = Attention(128, heads, dim_head)\n","        self.mid_block2 = ResBlock(128, 128)\n","\n","        self.ups = nn.ModuleList([\n","\n","            ## first upsampling stage\n","            nn.ModuleList([\n","                ResBlock(128 + 64, 128),\n","                ResBlock(128 + 64, 128),\n","                LinearAttention(128, heads, dim_head),\n","                UpBlock(128, 64)\n","            ]),\n","\n","            ## second upsampling stage\n","            nn.ModuleList([\n","                ResBlock(64 + 32, 64),\n","                ResBlock(64 + 32, 64),\n","                LinearAttention(64, heads, dim_head),\n","                UpBlock(64, 32)\n","            ]),\n","\n","            ## does not upsample\n","            nn.ModuleList([\n","                ResBlock(32 + 32, 32),\n","                ResBlock(32 + 32, 32),\n","                LinearAttention(32, heads, dim_head),\n","                ## MODIFY ##\n","                # nn.Conv1d(32, 32, kernel_size = 3, padding = 1)\n","                nn.Conv2d(32, 32, kernel_size = 3, padding = 1)\n","            ])\n","        ])\n","\n","        self.output_res = ResBlock(32 + 32, 32)\n","        self.output_layer = nn.Conv2d(32, in_channels, kernel_size = 1)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","\n","        ## MODIFY ##\n","        # b, c, l = x.shape\n","        b, c, height, width = x.shape\n","\n","        y = self.input_layer(x) ## (b x 32 x l)\n","        r = y.clone()\n","\n","        residuals = []\n","        for res1, res2, attention, downsample in self.downs:\n","            y = res1(y)\n","            residuals.append(y)\n","            y = res2(y)\n","            y = attention(y) + y\n","            residuals.append(y)\n","            y = downsample(y)\n","\n","        ## (b x 128 x l // 4)\n","        y = self.mid_block1(y)\n","        y = self.mid_attention(y) + y\n","        y = self.mid_block2(y)\n","\n","        for res1, res2, attention, upsample in self.ups:\n","            y = res1(torch.cat((y, residuals.pop()), dim = 1))\n","            y = res2(torch.cat((y, residuals.pop()), dim = 1))\n","            y = attention(y) + y\n","            y = upsample(y)\n","\n","        ## final skip connection to residual layer.\n","        y = self.output_res(torch.cat((y, r), dim = 1))\n","        y = self.output_layer(y)\n","        return y\n","\n","if __name__ == '__main__':\n","\n","    # model = UNet()\n","\n","    batch_size = 32\n","    in_channels = 2\n","    seq_len = 128\n","\n","    ## simulated input (audio spectrogram, financial timeseries, etc...)\n","    # x = torch.randn(batch_size, in_channels, seq_len)\n","\n","    ## call model on input\n","    # y = model(x)\n","\n","    # print(x.shape)\n","    # print(y.shape)\n","\n","    batch_size = 32\n","    in_channels = 3\n","    img_hight = 128\n","    img_width = 128\n","\n","    model = UNet(in_channels)\n","\n","    x = torch.randn(batch_size, in_channels, img_hight, img_width)\n","    print(x.shape)\n","\n","    y = model(x)\n","    print(x.shape, y.shape)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 3, 128, 128])\n","torch.Size([32, 3, 128, 128]) torch.Size([32, 3, 128, 128])\n"]}]},{"cell_type":"markdown","source":["2D_UNET"],"metadata":{"id":"gt8SYPlyhBN9"}},{"cell_type":"code","source":["'''\n","Code sources:\n","https://www.youtube.com/watch?v=ZBKpAp_6TGI&t=1841s\n","https://huggingface.co/blog/annotated-diffusion\n","'''\n","import torch\n","from torch import nn, Tensor, einsum\n","from einops.layers.torch import Rearrange\n","from einops import rearrange\n","\n","class DownBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels):\n","        super().__init__()\n","\n","        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2)\n","        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size = 3, padding = 1)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        return self.conv(self.rearrange(x))\n","\n","class UpBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","        self.upsample = nn.Upsample(scale_factor = 2)\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        return self.conv(self.upsample(x))\n","\n","class Block(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n","        self.norm = nn.GroupNorm(8, out_channels)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        x = self.norm(self.conv(x))\n","        return self.act(x)\n","\n","class ResBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int):\n","        super().__init__()\n","\n","        self.block1 = Block(in_channels, out_channels)\n","        self.block2 = Block(out_channels, out_channels)\n","        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n","\n","    def forward(self, x: Tensor):\n","        h = self.block1(x)\n","        return self.block2(h) + self.skip(x)\n","\n","class RMSNorm(nn.Module):\n","    def __init__(self, in_channels: int):\n","        super().__init__()\n","        self.g = nn.Parameter(torch.ones(1, in_channels, 1, 1))\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        ## pytorch normalize takes the signal and computes x / sum(x ** 2, dim = 1).sqrt()\n","        ## we want x / (sum(x ** 2, dim = 1) / sqrt(num_channels))\n","        return nn.functional.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n","\n","## https://huggingface.co/blog/annotated-diffusion\n","class LinearAttention(nn.Module):\n","    '''\n","    Computes attention that scales linearly in memory O(n) rather than polynomial O(n^2).\n","\n","    https://arxiv.org/pdf/2006.16236\n","    '''\n","    def __init__(self, in_channels: int, heads: int, dim_head: int):\n","        super().__init__()\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        hidden_dim = heads * dim_head\n","\n","        self.norm = RMSNorm(in_channels)\n","        self.qkv = nn.Conv2d(in_channels, 3*hidden_dim, kernel_size = 1, bias = False)\n","        self.output = nn.Sequential(\n","            nn.Conv2d(hidden_dim, in_channels, kernel_size = 1),\n","            RMSNorm(in_channels)\n","        )\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        b, c, height, width = x.shape\n","        ## in the einsums i corresponds to height and j corresponds to width\n","\n","        x = self.norm(x)\n","        qkv = self.qkv(x)\n","        q, k, v = qkv.chunk(3, dim = 1)\n","\n","        ## takes the channel dimention and reshapes into heads, channels\n","        ## also flattens the feature maps into vectors\n","        ## the shape is now b h c d where d is dim_head\n","        q = rearrange(q, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","        k = rearrange(k, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","        v = rearrange(v, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","        ## softmax along dim_head dim\n","        q = q.softmax(dim = 2) * self.scale\n","        ## softmax along flattened image dim\n","        k = k.softmax(dim = 3)\n","        ## compute comparison betweeen keys and values to produce context.\n","        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n","        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n","        out = rearrange(out, 'b h c (i j) -> b (h c) i j', i = height, j = width)\n","        return self.output(out)\n","\n","## https://huggingface.co/blog/annotated-diffusion\n","class Attention(nn.Module):\n","    '''\n","    Computes full pixelwise attention.\n","    '''\n","    def __init__(self, in_channels: int, heads: int, dim_head: int):\n","        super().__init__()\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","        hidden_dim = heads * dim_head\n","\n","        self.norm = RMSNorm(in_channels)\n","        self.qkv = nn.Conv2d(in_channels, hidden_dim * 3, kernel_size = 1, bias = False)\n","        self.output = nn.Sequential(\n","            nn.Conv2d(hidden_dim, in_channels, kernel_size = 1),\n","            RMSNorm(in_channels)\n","        )\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        b, c, height, width = x.shape\n","\n","        x = self.norm(x)\n","\n","        ## compute the queries, keys, and values of the incoming feature maps\n","        q, k, v = torch.chunk(self.qkv(x), 3, dim = 1)\n","        ## takes the channel dimention and reshapes into heads, channels\n","\n","        q = rearrange(q, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","        k = rearrange(k, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","        v = rearrange(v, 'b (h c) i j -> b h c (i j)', h = self.heads)\n","        q = q * self.scale\n","        ## multiplication of the query and key matrixes for each head\n","        sim = einsum('b h c i, b h c j -> b h i j', q, k)\n","        ## subtract the maximum value of each row\n","        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n","        ## make each row into a probability distribution\n","        attn = sim.softmax(dim = -1)\n","        ## weight the values according to the rows of the attention matrix\n","        y = einsum('b h i j, b h d j -> b h i d', attn, v)\n","        # reshape the weighted values back into feature maps\n","        y = rearrange(y, 'b h (i j) d -> b (h d) i j', i = height, j = width)\n","        return self.output(y)\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels: int = 2, heads: int = 4, dim_head: int = 32):\n","        super().__init__()\n","\n","        self.input_layer = nn.Conv2d(in_channels, 32, kernel_size = 1)\n","\n","        self.downs = nn.ModuleList([\n","            nn.ModuleList([\n","                ResBlock(32, 32),\n","                ResBlock(32, 32),\n","                LinearAttention(32, heads, dim_head),\n","                DownBlock(32, 32)\n","            ]),\n","\n","            nn.ModuleList([\n","                ResBlock(32, 32),\n","                ResBlock(32, 32),\n","                LinearAttention(32, heads, dim_head),\n","                DownBlock(32, 64)\n","            ]),\n","\n","            nn.ModuleList([\n","                ResBlock(64, 64),\n","                ResBlock(64, 64),\n","                LinearAttention(64, heads, dim_head),\n","                nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n","            ])\n","        ])\n","\n","        self.mid_block1 = ResBlock(128, 128)\n","        self.mid_attention = Attention(128, heads, dim_head)\n","        self.mid_block2 = ResBlock(128, 128)\n","\n","        self.ups = nn.ModuleList([\n","            nn.ModuleList([\n","                ResBlock(128 + 64, 128),\n","                ResBlock(128 + 64, 128),\n","                LinearAttention(128, heads, dim_head),\n","                UpBlock(128, 64)\n","            ]),\n","\n","            nn.ModuleList([\n","                ResBlock(64 + 32, 64),\n","                ResBlock(64 + 32, 64),\n","                LinearAttention(64, heads, dim_head),\n","                UpBlock(64, 32)\n","            ]),\n","\n","            nn.ModuleList([\n","                ResBlock(32 + 32, 32),\n","                ResBlock(32 + 32, 32),\n","                LinearAttention(32, heads, dim_head),\n","                nn.Conv2d(32, 32, kernel_size = 3, padding = 1)\n","            ])\n","        ])\n","\n","        self.output_res = ResBlock(32 + 32, 32)\n","        self.output_layer = nn.Conv2d(32, in_channels, kernel_size = 1)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        b, c, height, width = x.shape\n","\n","        y = self.input_layer(x) ## (b x 32 x height x width)\n","        r = y.clone()\n","\n","        residuals = []\n","        for res1, res2, attention, downsample in self.downs:\n","            y = res1(y)\n","            residuals.append(y)\n","            y = res2(y)\n","            y = attention(y) + y\n","            residuals.append(y)\n","            y = downsample(y)\n","\n","        y = self.mid_block1(y)\n","        y = self.mid_attention(y) + y\n","        y = self.mid_block2(y)\n","\n","        for res1, res2, attention, upsample in self.ups:\n","            y = res1(torch.cat((y, residuals.pop()), dim = 1))\n","            y = res2(torch.cat((y, residuals.pop()), dim = 1))\n","            y = attention(y) + y\n","            y = upsample(y)\n","\n","        ## final skip connection to residual layer\n","        y = self.output_res(torch.cat((y, r), dim = 1))\n","        y = self.output_layer(y)\n","        return y\n","\n","if __name__ == '__main__':\n","\n","    batch_size = 32\n","    in_channels = 3\n","    img_hight = 128\n","    img_width = 128\n","\n","    model = UNet(in_channels)\n","\n","    x = torch.randn(batch_size, in_channels, img_hight, img_width)\n","    print(x.shape)\n","\n","    y = model(x)\n","    print(x.shape, y.shape)"],"metadata":{"id":"OoSpnntthPbQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"38a6033a-970a-4ccb-c075-0052fad51089"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 3, 128, 128])\n"]}]}]}